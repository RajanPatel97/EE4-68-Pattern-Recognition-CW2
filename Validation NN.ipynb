{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#Store Data Variables\n",
    "import json\n",
    "with open('feature_data.json', 'r') as f:\n",
    " features = json.load(f)\n",
    "\n",
    "from scipy.io import loadmat\n",
    "train_idxs = loadmat('cuhk03_new_protocol_config_labeled.mat')['train_idx'].flatten()\n",
    "query_idxs = loadmat('cuhk03_new_protocol_config_labeled.mat')['query_idx'].flatten()\n",
    "labels = loadmat('cuhk03_new_protocol_config_labeled.mat')['labels'].flatten()\n",
    "gallery_idxs = loadmat('cuhk03_new_protocol_config_labeled.mat')['gallery_idx'].flatten()\n",
    "filelist = loadmat('cuhk03_new_protocol_config_labeled.mat')['filelist'].flatten()\n",
    "camId = loadmat('cuhk03_new_protocol_config_labeled.mat')['camId'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     2,     3, ..., 14094, 14095, 14096], dtype=uint16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   22,    27,    49, ..., 14043, 14059, 14063], dtype=uint16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    1,    1, ..., 1467, 1467, 1467], dtype=uint16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   21,    23,    24, ..., 14062, 14064, 14065], dtype=uint16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gallery_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array(['1_001_1_01.png'], dtype='<U14'),\n",
       "       array(['1_001_1_02.png'], dtype='<U14'),\n",
       "       array(['1_001_1_03.png'], dtype='<U14'), ...,\n",
       "       array(['5_049_2_08.png'], dtype='<U14'),\n",
       "       array(['5_049_2_09.png'], dtype='<U14'),\n",
       "       array(['5_049_2_10.png'], dtype='<U14')], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 2, 2, 2], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid searc cv against k, distance metric and transformation/normalization - use only query and gallery for this part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13791114 1.12505555 0.05404324 ... 0.10747871 0.04081609 0.68009406]\n",
      " [0.02507781 0.9057585  0.00346441 ... 0.16763815 0.14764351 0.2700713 ]\n",
      " [0.09923808 1.09122825 0.01889733 ... 0.07981343 0.04958951 0.37923682]\n",
      " ...\n",
      " [0.50913167 2.1067946  1.12224829 ... 0.78767842 1.68007588 0.00259321]\n",
      " [0.44684452 1.87411916 1.51910186 ... 1.02090526 1.58616257 0.10876646]\n",
      " [0.46024311 2.31024432 1.20531154 ... 0.62753201 1.22394812 0.        ]]\n",
      "[[0.13791114 1.12505555 0.05404324 ... 0.10747871 0.04081609 0.68009406]\n",
      " [0.02507781 0.9057585  0.00346441 ... 0.16763815 0.14764351 0.2700713 ]\n",
      " [0.09923808 1.09122825 0.01889733 ... 0.07981343 0.04958951 0.37923682]\n",
      " ...\n",
      " [0.50913167 2.1067946  1.12224829 ... 0.78767842 1.68007588 0.00259321]\n",
      " [0.44684452 1.87411916 1.51910186 ... 1.02090526 1.58616257 0.10876646]\n",
      " [0.46024311 2.31024432 1.20531154 ... 0.62753201 1.22394812 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#scaler = StandardScaler()\n",
    "print(np.array(features))\n",
    "#features = scaler.fit_transform(features)\n",
    "X = np.array(features)\n",
    "print(X)\n",
    "y = np.array(labels)\n",
    "filelist = np.array(filelist)\n",
    "camId = np.array(camId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = np.array(train_idxs).ravel()\n",
    "mask_query = np.array(query_idxs).ravel()\n",
    "mask_gallery = np.array(gallery_idxs).ravel()\n",
    "\n",
    "mask_train = np.subtract(mask_train, 1)\n",
    "mask_query = np.subtract(mask_query, 1)\n",
    "mask_gallery = np.subtract(mask_gallery, 1)\n",
    "\n",
    "\n",
    "X_train, X_query, X_gallery = X[mask_train, :], X[mask_query, :], X[mask_gallery, :]\n",
    "y_train, y_query, y_gallery = y[mask_train], y[mask_query], y[mask_gallery]\n",
    "filelist_train, filelist_query, filelist_gallery = filelist[mask_train], filelist[mask_query], filelist[mask_gallery]\n",
    "camId_train, camId_query, camId_gallery = camId[mask_train], camId[mask_query], camId[mask_gallery]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7368"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5328"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_gallery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7368"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = []\n",
    "y_val = []\n",
    "camId_val = []\n",
    "val_ind = []\n",
    "for i in range(7368):\n",
    "        if(i not in val_ind):\n",
    "            X_val.append(X_train[i])\n",
    "            y_val.append(y_train[i])\n",
    "            camId_val.append(camId_train[i])\n",
    "            val_ind.append(i)\n",
    "            for j in range(7368):\n",
    "                if(y_train[i] == y_train[j] and i != j):\n",
    "                    X_val.append(X_train[j])\n",
    "                    y_val.append(y_train[j])\n",
    "                    camId_val.append(camId_train[j])\n",
    "                    val_ind.append(j)\n",
    "            if ((len(set(y_val)) > 99)):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "966"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "966"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "966"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(camId_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "966"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(val_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = []\n",
    "y_train_new = []\n",
    "camId_train_new = []\n",
    "for i in range(7368):\n",
    "    if(i not in val_ind):\n",
    "        X_train_new.append(X_train[i])\n",
    "        y_train_new.append(y_train[i])\n",
    "        camId_train_new.append(camId_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6402"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6402"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6402"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(camId_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from keras import layers, optimizers, regularizers\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.utils import plot_model\n",
    "#from kt_utils import *\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn import preprocessing, model_selection \n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "# layer 1\n",
    "model.add(Dense(6144, input_dim=6144, activation='relu', kernel_initializer='normal'))\n",
    "#layer 2\n",
    "model.add(Dense(1024, activation='relu', kernel_initializer='normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "#layer 3\n",
    "model.add(Dense(128, activation='relu',kernel_initializer='normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "#layer 4\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# Compile model\n",
    "from keras import metrics\n",
    "optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer = optimizer, loss='categorical_crossentropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6402/6402 [==============================] - 55s 9ms/step - loss: 2.0674\n",
      "Epoch 2/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.6067\n",
      "Epoch 3/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.4528\n",
      "Epoch 4/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.4029\n",
      "Epoch 5/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.3887\n",
      "Epoch 6/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.3855\n",
      "Epoch 7/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.3849\n",
      "Epoch 8/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.3848\n",
      "Epoch 9/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.3848\n",
      "Epoch 10/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.3848\n",
      "Epoch 11/20\n",
      "6402/6402 [==============================] - 45s 7ms/step - loss: 1.3848\n",
      "Epoch 12/20\n",
      "6402/6402 [==============================] - 45s 7ms/step - loss: 1.3848\n",
      "Epoch 13/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.3848\n",
      "Epoch 14/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.3848\n",
      "Epoch 15/20\n",
      "6402/6402 [==============================] - 45s 7ms/step - loss: 1.3848\n",
      "Epoch 16/20\n",
      "6402/6402 [==============================] - 45s 7ms/step - loss: 1.3848\n",
      "Epoch 17/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.3848\n",
      "Epoch 18/20\n",
      "6402/6402 [==============================] - 44s 7ms/step - loss: 1.3848\n",
      "Epoch 19/20\n",
      "6402/6402 [==============================] - 45s 7ms/step - loss: 1.3848\n",
      "Epoch 20/20\n",
      "6402/6402 [==============================] - 45s 7ms/step - loss: 1.3848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f6dcfcc1d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pairs = []\n",
    "y_train_pair_lables = []\n",
    "for Xtnn, ytnn in zip(X_train_new, y_train_new):\n",
    "    randindex = random.randint(0, 6401)\n",
    "    Xtnn2 = X_train[randindex]\n",
    "    ytnn2 = y_train[randindex]\n",
    "    \n",
    "    randindex = random.randint(0, 6401)\n",
    "    Xtnn3 = X_train[randindex]\n",
    "    ytnn3 = y_train[randindex]                 \n",
    "    \n",
    "    if((ytnn == ytnn2) and (ytnn == ytnn3)):\n",
    "        y_train_pair_lables.append((0,0))\n",
    "    if((ytnn == ytnn2) and (ytnn != ytnn3)):\n",
    "        y_train_pair_lables.append((0,1))\n",
    "    if((ytnn != ytnn2) and (ytnn == ytnn3)):\n",
    "        y_train_pair_lables.append((1,0))\n",
    "    if((ytnn != ytnn2) and (ytnn != ytnn3)):\n",
    "        y_train_pair_lables.append((1,1))\n",
    "    \n",
    "    Xconcat = np.concatenate((Xtnn,Xtnn2,Xtnn3), axis = None)\n",
    "    X_train_pairs.append(Xconcat)\n",
    "\n",
    "y_train_pair_lables = np.array(y_train_pair_lables)\n",
    "X_train_pairs = np.array(X_train_pairs)\n",
    "model.fit(X_train_pairs, y_train_pair_lables, batch_size = 80, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_score(y_valid, y_q, tot_label_occur):\n",
    "    recall = 0\n",
    "    true_positives = 0\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    max_rank = 30\n",
    "    \n",
    "    rank_A = np.zeros(max_rank)\n",
    "    AP_arr = np.zeros(11)\n",
    "    \n",
    "    while (recall < 1) or (k < max_rank):\n",
    "        if (y_valid[k] == y_q):\n",
    "            \n",
    "            true_positives = true_positives + 1\n",
    "            recall = true_positives/tot_label_occur\n",
    "            precision = true_positives/(k+1)\n",
    "            \n",
    "            AP_arr[round((recall-0.05)*10)] = precision\n",
    "            \n",
    "            for n in range (k, max_rank):\n",
    "                rank_A[n] = 1\n",
    "            \n",
    "        k = k+1\n",
    "        \n",
    "    max_precision = 0\n",
    "    for i in range(10, -1, -1):\n",
    "        max_precision = max(max_precision, AP_arr[i])\n",
    "        AP_arr[i] = max_precision\n",
    "    \n",
    "    AP_ = AP_arr.sum()/11\n",
    "    \n",
    "    return AP_, rank_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49918127\n",
      "0.49957326\n",
      "0.5004536\n",
      "0.49922138\n",
      "0.4998859\n",
      "0.49856836\n",
      "0.49893686\n",
      "0.49894884\n",
      "0.49846354\n",
      "0.4987743\n",
      "0.49796966\n",
      "0.49981484\n",
      "0.4994514\n",
      "0.4990767\n",
      "0.4985363\n",
      "0.49959022\n",
      "0.49910438\n",
      "0.5003375\n",
      "0.4992975\n",
      "0.50031084\n",
      "0.49992296\n",
      "0.49945357\n",
      "0.49938747\n",
      "0.5004347\n",
      "0.5003014\n",
      "0.5014467\n",
      "0.49984035\n",
      "0.50015026\n",
      "0.50113404\n",
      "0.49989155\n",
      "0.50064677\n",
      "0.50099456\n",
      "0.500833\n",
      "0.50060064\n",
      "0.49870646\n",
      "0.499084\n",
      "0.5006839\n",
      "0.49882784\n",
      "0.4989555\n",
      "0.49975643\n",
      "0.49913037\n",
      "0.49902642\n",
      "0.4987\n",
      "0.49968123\n",
      "0.49938253\n",
      "0.4994635\n",
      "0.49981728\n",
      "0.50019\n",
      "0.5007972\n",
      "0.4988809\n",
      "0.49960738\n",
      "0.49906567\n",
      "0.49994755\n",
      "0.49962124\n",
      "0.5012182\n",
      "0.50039107\n",
      "0.5010477\n",
      "0.50012565\n",
      "0.50053835\n",
      "0.5004243\n",
      "0.4997932\n",
      "0.5002656\n",
      "0.5011556\n",
      "0.4990385\n",
      "0.49801382\n",
      "0.4998822\n",
      "0.49948218\n",
      "0.49920556\n",
      "0.4999834\n",
      "0.5002406\n",
      "0.49966133\n",
      "0.49884707\n",
      "0.49995813\n",
      "0.4998128\n",
      "0.5004763\n",
      "0.50086766\n",
      "0.50020427\n",
      "0.49997073\n",
      "0.50000435\n",
      "0.5002283\n",
      "0.49989033\n",
      "0.4992723\n",
      "0.49955782\n",
      "0.4980214\n",
      "0.49915996\n",
      "0.49917033\n",
      "0.4992385\n",
      "0.498603\n",
      "0.49842182\n",
      "0.49800673\n",
      "0.49907085\n",
      "0.49849582\n",
      "0.49994954\n",
      "0.49920338\n",
      "0.49914172\n",
      "0.49973112\n",
      "0.49913463\n",
      "0.49833864\n",
      "0.49944705\n",
      "0.4988903\n",
      "0.4982275\n",
      "0.49809176\n",
      "0.4992995\n",
      "0.49917153\n",
      "0.4994569\n",
      "0.49926114\n",
      "0.49920934\n",
      "0.49927223\n",
      "0.5012268\n",
      "0.5008705\n",
      "0.49924806\n",
      "0.50004905\n",
      "0.49953803\n",
      "0.5009574\n",
      "0.50015444\n",
      "0.5010164\n",
      "0.50014186\n",
      "0.4993309\n",
      "0.50030243\n",
      "0.49940142\n",
      "0.49898714\n",
      "0.49976304\n",
      "0.49940962\n",
      "0.4982991\n",
      "0.4986652\n",
      "0.49877828\n",
      "0.49995306\n",
      "0.49889803\n",
      "0.49988475\n",
      "0.49932674\n",
      "0.49949983\n",
      "0.4999014\n",
      "0.5000393\n",
      "0.49942842\n",
      "0.4992109\n",
      "0.50051844\n",
      "0.5006962\n",
      "0.49994174\n",
      "0.49957743\n",
      "0.49885502\n",
      "0.4978788\n",
      "0.50046515\n",
      "0.4984799\n",
      "0.49984565\n",
      "0.4996895\n",
      "0.49883717\n",
      "0.49990144\n",
      "0.49927145\n",
      "0.49795341\n",
      "0.4995088\n",
      "0.49925408\n",
      "0.49980295\n",
      "0.49828547\n",
      "0.4990654\n",
      "0.4995383\n",
      "0.49999878\n",
      "0.4991618\n",
      "0.49919638\n",
      "0.5000341\n",
      "0.49858728\n",
      "0.49996316\n",
      "0.50042754\n",
      "0.49886054\n",
      "0.4989416\n",
      "0.500162\n",
      "0.500623\n",
      "0.4996604\n",
      "0.49955085\n",
      "0.50101554\n",
      "0.50036603\n",
      "0.49995536\n",
      "0.5012068\n",
      "0.5003931\n",
      "0.49949643\n",
      "0.50009394\n",
      "0.4985987\n",
      "0.50040764\n",
      "0.5008166\n",
      "0.49876085\n",
      "0.50039023\n",
      "0.5010047\n",
      "0.49924734\n",
      "0.50063795\n",
      "0.500332\n",
      "0.49974036\n",
      "0.49972123\n",
      "0.49838522\n",
      "0.49984375\n",
      "0.49869528\n",
      "0.5002557\n",
      "0.49867234\n",
      "0.49994412\n",
      "0.50088555\n",
      "0.500144\n",
      "0.50069314\n",
      "0.4992502\n",
      "0.4999636\n",
      "0.5010399\n",
      "0.501373\n",
      "0.49997547\n",
      "0.4997227\n",
      "0.49959716\n",
      "0.50002074\n",
      "0.49994823\n",
      "0.50004166\n",
      "0.49995667\n",
      "0.5002921\n",
      "0.49915808\n",
      "0.49997142\n",
      "0.500136\n",
      "0.50057256\n",
      "0.49899873\n",
      "0.49980912\n",
      "0.499249\n",
      "0.49976704\n",
      "0.4993236\n",
      "0.49860415\n",
      "0.49918798\n",
      "0.49954802\n",
      "0.49992427\n",
      "0.499024\n",
      "0.49934125\n",
      "0.5000718\n",
      "0.49951047\n",
      "0.4992915\n",
      "0.4996282\n",
      "0.4994678\n",
      "0.49942604\n",
      "0.50035\n",
      "0.50007784\n",
      "0.5002711\n",
      "0.49996796\n",
      "0.49972817\n",
      "0.49991772\n",
      "0.50045484\n",
      "0.49981272\n",
      "0.49946642\n",
      "0.4990499\n",
      "0.49979135\n",
      "0.4986818\n",
      "0.50074035\n",
      "0.498698\n",
      "0.49903497\n",
      "0.4995337\n",
      "0.49931473\n",
      "0.4989733\n",
      "0.50009745\n",
      "0.50112027\n",
      "0.49950877\n",
      "0.50032187\n",
      "0.5004818\n",
      "0.49919823\n",
      "0.5000514\n",
      "0.4998554\n",
      "0.49910358\n",
      "0.5004644\n",
      "0.5005033\n",
      "0.49937472\n",
      "0.5002855\n",
      "0.49887785\n",
      "0.5004302\n",
      "0.50039685\n",
      "0.49948972\n",
      "0.49807614\n",
      "0.49898475\n",
      "0.49878237\n",
      "0.4995083\n",
      "0.4999273\n",
      "0.498996\n",
      "0.49946758\n",
      "0.49877822\n",
      "0.4989257\n",
      "0.49869975\n",
      "0.4999545\n",
      "0.49920303\n",
      "0.4983639\n",
      "0.4989924\n",
      "0.49908194\n",
      "0.49844682\n",
      "0.50049174\n",
      "0.49991992\n",
      "0.49961725\n",
      "0.4991512\n",
      "0.49992552\n",
      "0.49906787\n",
      "0.49942198\n",
      "0.4999309\n",
      "0.4999118\n",
      "0.499863\n",
      "0.4999337\n",
      "0.5009295\n",
      "0.5002782\n",
      "0.50063676\n",
      "0.5016413\n",
      "0.49926528\n",
      "0.49964285\n",
      "0.49903333\n",
      "0.5016647\n",
      "0.50160116\n",
      "0.5007193\n",
      "0.49976775\n",
      "0.50028586\n",
      "0.5009962\n",
      "0.50131494\n",
      "0.49976936\n",
      "0.49960175\n",
      "0.5002412\n",
      "0.50040174\n",
      "0.49956143\n",
      "0.4992532\n",
      "0.50078666\n",
      "0.49995443\n",
      "0.5000838\n",
      "0.501707\n",
      "0.5013783\n",
      "0.49990615\n",
      "0.49981654\n",
      "0.49894375\n",
      "0.5011484\n",
      "0.4992821\n",
      "0.4997079\n",
      "0.50042164\n",
      "0.50005674\n",
      "0.49948648\n",
      "0.49931622\n",
      "0.49966177\n",
      "0.49976143\n",
      "0.49965376\n",
      "0.4997154\n",
      "0.4994506\n",
      "0.49878508\n",
      "0.49958375\n",
      "0.49971426\n",
      "0.50037473\n",
      "0.4997229\n",
      "0.4995003\n",
      "0.4989662\n",
      "0.4991255\n",
      "0.50054246\n",
      "0.49997962\n",
      "0.4999236\n",
      "0.50111806\n",
      "0.49987087\n",
      "0.500186\n",
      "0.49976376\n",
      "0.5013946\n",
      "0.49889627\n",
      "0.49973226\n",
      "0.49959022\n",
      "0.50199443\n",
      "0.49978298\n",
      "0.5006583\n",
      "0.5010895\n",
      "0.49968457\n",
      "0.5017351\n",
      "0.50110424\n",
      "0.5018118\n",
      "0.5002337\n",
      "0.5002747\n",
      "0.4994337\n",
      "0.49988738\n",
      "0.4984345\n",
      "0.49882817\n",
      "0.49990958\n",
      "0.49908546\n",
      "0.49959698\n",
      "0.5006423\n",
      "0.49944377\n",
      "0.50071675\n",
      "0.49941078\n",
      "0.4999731\n",
      "0.50045484\n",
      "0.49883437\n",
      "0.50043803\n",
      "0.5001198\n",
      "0.49967623\n",
      "0.500016\n",
      "0.50064826\n",
      "0.5010803\n",
      "0.50160307\n",
      "0.5000611\n",
      "0.50108624\n",
      "0.5010027\n",
      "0.50066435\n",
      "0.50045884\n",
      "0.5007003\n",
      "0.50041956\n",
      "0.49852088\n",
      "0.49961543\n",
      "0.49948972\n",
      "0.49867272\n",
      "0.4996498\n",
      "0.49791914\n",
      "0.49981222\n",
      "0.49869224\n",
      "0.4992667\n",
      "0.49893144\n",
      "0.49843973\n",
      "0.4984377\n",
      "0.49846455\n",
      "0.49960545\n",
      "0.49891543\n",
      "0.49950972\n",
      "0.50095886\n",
      "0.49984193\n",
      "0.5005771\n",
      "0.5008296\n",
      "0.49992713\n",
      "0.50079155\n",
      "0.5004809\n",
      "0.50048274\n",
      "0.5012053\n",
      "0.5001844\n",
      "0.4985962\n",
      "0.4992475\n",
      "0.49955595\n",
      "0.49888927\n",
      "0.49920285\n",
      "0.4996896\n",
      "0.49819532\n",
      "0.49866197\n",
      "0.49939317\n",
      "0.50010455\n",
      "0.49844122\n",
      "0.49974006\n",
      "0.4996661\n",
      "0.49895442\n",
      "0.498622\n",
      "0.49980253\n",
      "0.5007128\n",
      "0.4997489\n",
      "0.49932614\n",
      "0.5013511\n",
      "0.5008191\n",
      "0.50084484\n",
      "0.50072765\n",
      "0.5000898\n",
      "0.50065845\n",
      "0.5001783\n",
      "0.5001215\n",
      "0.50171053\n",
      "0.50081897\n",
      "0.49984843\n",
      "0.49953425\n",
      "0.49875632\n",
      "0.499416\n",
      "0.4997615\n",
      "0.4989939\n",
      "0.500506\n",
      "0.49912098\n",
      "0.49973834\n",
      "0.4986079\n",
      "0.49871317\n",
      "0.49916196\n",
      "0.50022924\n",
      "0.4994167\n",
      "0.49960038\n",
      "0.5003551\n",
      "0.49980474\n",
      "0.49930754\n",
      "0.49945888\n",
      "0.5001063\n",
      "0.49954057\n",
      "0.50050557\n",
      "0.5006891\n",
      "0.5009616\n",
      "0.50078297\n",
      "0.5009909\n",
      "0.50112635\n",
      "0.5015738\n",
      "0.5008794\n",
      "0.50064504\n",
      "0.49948695\n",
      "0.49970213\n",
      "0.5001211\n",
      "0.4992602\n",
      "0.5006633\n",
      "0.4991849\n",
      "0.50016266\n",
      "0.5002251\n",
      "0.50052035\n",
      "0.5015669\n",
      "0.5004275\n",
      "0.500473\n",
      "0.50072676\n",
      "0.50025743\n",
      "0.5014344\n",
      "0.5003485\n",
      "0.5004464\n",
      "0.5001608\n",
      "0.5006424\n",
      "0.49894089\n",
      "0.49955434\n",
      "0.49990016\n",
      "0.49922913\n",
      "0.49993086\n",
      "0.49979326\n",
      "0.50052774\n",
      "0.4994101\n",
      "0.5000361\n",
      "0.5008994\n",
      "0.49936956\n",
      "0.5012721\n",
      "0.49972963\n",
      "0.50098485\n",
      "0.4998886\n",
      "0.50147307\n",
      "0.49781737\n",
      "0.49963564\n",
      "0.4990535\n",
      "0.49879298\n",
      "0.49999946\n",
      "0.49983218\n",
      "0.49860314\n",
      "0.49837646\n",
      "0.49918306\n",
      "0.5003471\n",
      "0.50099117\n",
      "0.50160295\n",
      "0.5011296\n",
      "0.50051904\n",
      "0.5020234\n",
      "0.5023099\n",
      "0.5009593\n",
      "0.5009212\n",
      "0.5006239\n",
      "0.5019655\n",
      "0.4992147\n",
      "0.5005485\n",
      "0.50056505\n",
      "0.499617\n",
      "0.4988654\n",
      "0.49904805\n",
      "0.49987012\n",
      "0.49883616\n",
      "0.500625\n",
      "0.49900258\n",
      "0.50015503\n",
      "0.5002214\n",
      "0.49994484\n",
      "0.5001801\n",
      "0.49980015\n",
      "0.4994893\n",
      "0.5001264\n",
      "0.49906376\n",
      "0.4991738\n",
      "0.5003559\n",
      "0.49777997\n",
      "0.49916482\n",
      "0.49892431\n",
      "0.49956962\n",
      "0.49880737\n",
      "0.49921966\n",
      "0.49937353\n",
      "0.49919516\n",
      "0.499683\n",
      "0.49863505\n",
      "0.50019795\n",
      "0.50000536\n",
      "0.50006425\n",
      "0.49949855\n",
      "0.50084543\n",
      "0.5010724\n",
      "0.49983975\n",
      "0.50096184\n",
      "0.50085646\n",
      "0.49968576\n",
      "0.49995124\n",
      "0.5002353\n",
      "0.50131965\n",
      "0.50053394\n",
      "0.502011\n",
      "0.50057244\n",
      "0.5008178\n",
      "0.50006324\n",
      "0.50021625\n",
      "0.5000405\n",
      "0.5010498\n",
      "0.5007044\n",
      "0.50118846\n",
      "0.50088274\n",
      "0.5005272\n",
      "0.5011219\n",
      "0.50061655\n",
      "0.5010364\n",
      "0.5000375\n",
      "0.5008239\n",
      "0.5012164\n",
      "0.5001331\n",
      "0.5009146\n",
      "0.49954244\n",
      "0.50137526\n",
      "0.50004816\n",
      "0.5009293\n",
      "0.500378\n",
      "0.5006124\n",
      "0.50073665\n",
      "0.5017627\n",
      "0.50103015\n",
      "0.50112695\n",
      "0.50127476\n",
      "0.50046164\n",
      "0.5015849\n",
      "0.5009159\n",
      "0.5003269\n",
      "0.5012621\n",
      "0.5004909\n",
      "0.49973416\n",
      "0.49992627\n",
      "0.50007975\n",
      "0.5002333\n",
      "0.5002228\n",
      "0.50139564\n",
      "0.50040686\n",
      "0.5002798\n",
      "0.50010157\n",
      "0.50068873\n",
      "0.49884564\n",
      "0.4994254\n",
      "0.5010871\n",
      "0.5002632\n",
      "0.4996239\n",
      "0.49874693\n",
      "0.49917653\n",
      "0.49979687\n",
      "0.49834692\n",
      "0.4997011\n",
      "0.5014961\n",
      "0.5019016\n",
      "0.5005377\n",
      "0.5014628\n",
      "0.50118893\n",
      "0.5005451\n",
      "0.5013584\n",
      "0.50062233\n",
      "0.50156605\n",
      "0.5007655\n",
      "0.49900627\n",
      "0.49947664\n",
      "0.4985811\n",
      "0.49919817\n",
      "0.49912146\n",
      "0.49979004\n",
      "0.4997843\n",
      "0.50004303\n",
      "0.49966517\n",
      "0.4991739\n",
      "0.50146306\n",
      "0.5023572\n",
      "0.5011336\n",
      "0.50173247\n",
      "0.5025231\n",
      "0.50176454\n",
      "0.5020814\n",
      "0.5016313\n",
      "0.5011814\n",
      "0.499572\n",
      "0.5008683\n",
      "0.5017259\n",
      "0.5015373\n",
      "0.50135535\n",
      "0.50075525\n",
      "0.50178844\n",
      "0.5010694\n",
      "0.5004806\n",
      "0.50041974\n",
      "0.50102717\n",
      "0.4997741\n",
      "0.49951676\n",
      "0.5000944\n",
      "0.49984005\n",
      "0.49912608\n",
      "0.4999406\n",
      "0.49989897\n",
      "0.5006733\n",
      "0.49994755\n",
      "0.50019747\n",
      "0.5012427\n",
      "0.5001229\n",
      "0.50081825\n",
      "0.5013154\n",
      "0.50205934\n",
      "0.5014772\n",
      "0.50150985\n",
      "0.5020321\n",
      "0.5006954\n",
      "0.50097746\n",
      "0.49959973\n",
      "0.4986319\n",
      "0.49919686\n",
      "0.4997919\n",
      "0.49941298\n",
      "0.50054854\n",
      "0.49946284\n",
      "0.50049955\n",
      "0.4993686\n",
      "0.4991489\n",
      "0.50016934\n",
      "0.50088423\n",
      "0.50116986\n",
      "0.4992615\n",
      "0.4988352\n",
      "0.49947497\n",
      "0.49955297\n",
      "0.50005233\n",
      "0.49861744\n",
      "0.50024647\n",
      "0.5003081\n",
      "0.5013265\n",
      "0.50057876\n",
      "0.50037766\n",
      "0.50116086\n",
      "0.50099325\n",
      "0.5003643\n",
      "0.5004665\n",
      "0.49941266\n",
      "0.49968213\n",
      "0.5017226\n",
      "0.5013005\n",
      "0.5000546\n",
      "0.5008501\n",
      "0.5008576\n",
      "0.50087935\n",
      "0.5003225\n",
      "0.5001478\n",
      "0.5008006\n",
      "0.50068533\n",
      "0.50102204\n",
      "0.50006855\n",
      "0.50059074\n",
      "0.5002315\n",
      "0.49959517\n",
      "0.50068367\n",
      "0.50106925\n",
      "0.50029135\n",
      "0.50074947\n",
      "0.49999094\n",
      "0.49861372\n",
      "0.49998602\n",
      "0.4996481\n",
      "0.49966243\n",
      "0.5003831\n",
      "0.49932572\n",
      "0.49925393\n",
      "0.49958593\n",
      "0.49809006\n",
      "0.49904588\n",
      "0.4988352\n",
      "0.49902248\n",
      "0.4991041\n",
      "0.49946365\n",
      "0.49950692\n",
      "0.49935636\n",
      "0.49928123\n",
      "0.49871656\n",
      "0.49976078\n",
      "0.499806\n",
      "0.49926144\n",
      "0.49891475\n",
      "0.49945\n",
      "0.49964932\n",
      "0.49927413\n",
      "0.49922204\n",
      "0.49981707\n",
      "0.4994562\n",
      "0.49893782\n",
      "0.49911022\n",
      "0.50002664\n",
      "0.5001189\n",
      "0.49916884\n",
      "0.50029904\n",
      "0.49979088\n",
      "0.5003553\n",
      "0.49933043\n",
      "0.49995485\n",
      "0.5003172\n",
      "0.4999755\n",
      "0.4992087\n",
      "0.49879953\n",
      "0.500057\n",
      "0.49927777\n",
      "0.49945685\n",
      "0.4995727\n",
      "0.49899912\n",
      "0.49893463\n",
      "0.4991418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49750987\n",
      "0.49802446\n",
      "0.4985005\n",
      "0.49714273\n",
      "0.49876335\n",
      "0.4982849\n",
      "0.49872956\n",
      "0.49787012\n",
      "0.49754217\n",
      "0.49923688\n",
      "0.49838954\n",
      "0.50027484\n",
      "0.49976602\n",
      "0.5009574\n",
      "0.50017893\n",
      "0.4995782\n",
      "0.49988753\n",
      "0.49907893\n",
      "0.5004311\n",
      "0.5000751\n",
      "0.50140435\n",
      "0.5001427\n",
      "0.5008314\n",
      "0.5012388\n",
      "0.49993274\n",
      "0.5000555\n",
      "0.5003071\n",
      "0.50155854\n",
      "0.5025164\n",
      "0.49852237\n",
      "0.4993538\n",
      "0.49997196\n",
      "0.49906948\n",
      "0.49942747\n",
      "0.5000798\n",
      "0.50055623\n",
      "0.5001452\n",
      "0.49946016\n",
      "0.4991488\n",
      "0.50028694\n",
      "0.5005785\n",
      "0.50104\n",
      "0.50044125\n",
      "0.50109476\n",
      "0.5014348\n",
      "0.5021632\n",
      "0.5011114\n",
      "0.5004578\n",
      "0.49967533\n",
      "0.498451\n",
      "0.4994832\n",
      "0.49914145\n",
      "0.49948868\n",
      "0.49839458\n",
      "0.4993235\n",
      "0.5009241\n",
      "0.50090575\n",
      "0.50138605\n",
      "0.50024486\n",
      "0.50058144\n",
      "0.5009487\n",
      "0.5010979\n",
      "0.50040007\n",
      "0.49933442\n",
      "0.50029916\n",
      "0.49901876\n",
      "0.49883562\n",
      "0.49954614\n",
      "0.5005991\n",
      "0.5007906\n",
      "0.5006588\n",
      "0.50061977\n",
      "0.4994462\n",
      "0.49855468\n",
      "0.4999425\n",
      "0.5003311\n",
      "0.49926946\n",
      "0.50119\n",
      "0.50034475\n",
      "0.50030357\n",
      "0.50082517\n",
      "0.49991018\n",
      "0.50100946\n",
      "0.50045425\n",
      "0.500196\n",
      "0.49967694\n",
      "0.5011395\n",
      "0.5009017\n",
      "0.4999601\n",
      "0.49958155\n",
      "0.5002528\n",
      "0.49933773\n",
      "0.50068104\n",
      "0.49969256\n",
      "0.5001475\n",
      "0.5003066\n",
      "0.4996104\n",
      "0.50046057\n",
      "0.50071156\n",
      "0.500962\n",
      "0.49905884\n",
      "0.4999822\n",
      "0.5003222\n",
      "0.5002867\n",
      "0.4995647\n",
      "0.49897286\n",
      "0.49886546\n",
      "0.4987254\n",
      "0.49965873\n",
      "0.4979315\n",
      "0.50122154\n",
      "0.49967247\n",
      "0.49890777\n",
      "0.49940938\n",
      "0.49939463\n",
      "0.50011265\n",
      "0.49997196\n",
      "0.4998011\n",
      "0.499652\n",
      "0.4998796\n",
      "0.50124085\n",
      "0.5004032\n",
      "0.4993539\n",
      "0.5009769\n",
      "0.5000792\n",
      "0.5003952\n",
      "0.49912208\n",
      "0.49934176\n",
      "0.4998277\n",
      "0.50018317\n",
      "0.49925947\n",
      "0.5007134\n",
      "0.4998761\n",
      "0.49930122\n",
      "0.49902856\n",
      "0.50002134\n",
      "0.49900773\n",
      "0.498728\n",
      "0.4990273\n",
      "0.49976644\n",
      "0.49972486\n",
      "0.49982408\n",
      "0.5008765\n",
      "0.49949837\n",
      "0.500458\n",
      "0.49880126\n",
      "0.5000691\n",
      "0.49887455\n",
      "0.49733576\n",
      "0.49863434\n",
      "0.49908668\n",
      "0.49839443\n",
      "0.49854892\n",
      "0.49918988\n",
      "0.49799815\n",
      "0.49961755\n",
      "0.49867588\n",
      "0.50015324\n",
      "0.49967557\n",
      "0.49967116\n",
      "0.50006527\n",
      "0.5007231\n",
      "0.49951977\n",
      "0.50005734\n",
      "0.50042427\n",
      "0.49906674\n",
      "0.49960262\n",
      "0.49965808\n",
      "0.49952334\n",
      "0.4994324\n",
      "0.49953184\n",
      "0.49913827\n",
      "0.4980713\n",
      "0.49899948\n",
      "0.50013185\n",
      "[0.49714273 0.49733576 0.49750987 0.49754217 0.49777997 0.49781737\n",
      " 0.49787012 0.4978788  0.49791914 0.4979315  0.49795341 0.49796966\n",
      " 0.49799815 0.49800673 0.49801382 0.4980214  0.49802446 0.4980713\n",
      " 0.49807614 0.49809006 0.49809176 0.49819532 0.4982275  0.4982849\n",
      " 0.49828547 0.4982991  0.49833864 0.49834692 0.4983639  0.49837646\n",
      " 0.49838522 0.49838954 0.49839443 0.49839458 0.49842182 0.4984345\n",
      " 0.4984377  0.49843973 0.49844122 0.49844682 0.498451   0.49846354\n",
      " 0.49846455 0.4984799  0.49849582 0.4985005  0.49852088 0.49852237\n",
      " 0.4985363  0.49854892 0.49855468 0.49856836 0.4985811  0.49858728\n",
      " 0.4985962  0.4985987  0.498603   0.49860314 0.49860415 0.4986079\n",
      " 0.49861372 0.49861744 0.498622   0.4986319  0.49863434 0.49863505\n",
      " 0.49866197 0.4986652  0.49867234 0.49867272 0.49867588 0.4986818\n",
      " 0.49869224 0.49869528 0.498698   0.49869975 0.4987     0.49870646\n",
      " 0.49871317 0.49871656 0.4987254  0.498728   0.49872956 0.49874693\n",
      " 0.49875632 0.49876085 0.49876335 0.4987743  0.49877822 0.49877828\n",
      " 0.49878237 0.49878508 0.49879298 0.49879953 0.49880126 0.49880737\n",
      " 0.49882784 0.49882817 0.49883437 0.4988352  0.4988352  0.49883562\n",
      " 0.49883616 0.49883717 0.49884564 0.49884707 0.49885502 0.49886054\n",
      " 0.4988654  0.49886546 0.49887455 0.49887785 0.4988809  0.49888927\n",
      " 0.4988903  0.49889627 0.49889803 0.49890777 0.49891475 0.49891543\n",
      " 0.49892431 0.4989257  0.49893144 0.49893463 0.49893686 0.49893782\n",
      " 0.49894089 0.4989416  0.49894375 0.49894884 0.49895442 0.4989555\n",
      " 0.4989662  0.49897286 0.4989733  0.49898475 0.49898714 0.4989924\n",
      " 0.4989939  0.498996   0.49899873 0.49899912 0.49899948 0.49900258\n",
      " 0.49900627 0.49900773 0.49901876 0.49902248 0.499024   0.49902642\n",
      " 0.4990273  0.49902856 0.49903333 0.49903497 0.4990385  0.49904588\n",
      " 0.49904805 0.4990499  0.4990535  0.49905884 0.49906376 0.4990654\n",
      " 0.49906567 0.49906674 0.49906787 0.49906948 0.49907085 0.4990767\n",
      " 0.49907893 0.49908194 0.499084   0.49908546 0.49908668 0.49910358\n",
      " 0.4991041  0.49910438 0.49911022 0.49912098 0.49912146 0.49912208\n",
      " 0.4991255  0.49912608 0.49913037 0.49913463 0.49913827 0.49914145\n",
      " 0.49914172 0.4991418  0.4991488  0.4991489  0.4991512  0.49915808\n",
      " 0.49915996 0.4991618  0.49916196 0.49916482 0.49916884 0.49917033\n",
      " 0.49917153 0.4991738  0.4991739  0.49917653 0.49918127 0.49918306\n",
      " 0.4991849  0.49918798 0.49918988 0.49919516 0.49919638 0.49919686\n",
      " 0.49919817 0.49919823 0.49920285 0.49920303 0.49920338 0.49920556\n",
      " 0.4992087  0.49920934 0.4992109  0.4992147  0.49921966 0.49922138\n",
      " 0.49922204 0.49922913 0.49923688 0.4992385  0.49924734 0.4992475\n",
      " 0.49924806 0.499249   0.4992502  0.4992532  0.49925393 0.49925408\n",
      " 0.49925947 0.4992602  0.49926114 0.49926144 0.4992615  0.49926528\n",
      " 0.4992667  0.49926946 0.49927145 0.49927223 0.4992723  0.49927413\n",
      " 0.49927777 0.49928123 0.4992821  0.4992915  0.4992975  0.4992995\n",
      " 0.49930122 0.49930754 0.49931473 0.49931622 0.4993235  0.4993236\n",
      " 0.49932572 0.49932614 0.49932674 0.49933043 0.4993309  0.49933442\n",
      " 0.49933773 0.49934125 0.49934176 0.4993538  0.4993539  0.49935636\n",
      " 0.4993686  0.49936956 0.49937353 0.49937472 0.49938253 0.49938747\n",
      " 0.49939317 0.49939463 0.49940142 0.49940938 0.49940962 0.4994101\n",
      " 0.49941078 0.49941266 0.49941298 0.499416   0.4994167  0.49942198\n",
      " 0.4994254  0.49942604 0.49942747 0.49942842 0.4994324  0.4994337\n",
      " 0.49944377 0.4994462  0.49944705 0.49945    0.4994506  0.4994514\n",
      " 0.49945357 0.4994562  0.49945685 0.4994569  0.49945888 0.49946016\n",
      " 0.49946284 0.4994635  0.49946365 0.49946642 0.49946758 0.4994678\n",
      " 0.49947497 0.49947664 0.49948218 0.4994832  0.49948648 0.49948695\n",
      " 0.49948868 0.4994893  0.49948972 0.49948972 0.49949643 0.49949837\n",
      " 0.49949855 0.49949983 0.4995003  0.49950692 0.4995083  0.49950877\n",
      " 0.4995088  0.49950972 0.49951047 0.49951676 0.49951977 0.49952334\n",
      " 0.49953184 0.4995337  0.49953425 0.49953803 0.4995383  0.49954057\n",
      " 0.49954244 0.49954614 0.49954802 0.49955085 0.49955297 0.49955434\n",
      " 0.49955595 0.49955782 0.49956143 0.4995647  0.49956962 0.499572\n",
      " 0.4995727  0.49957326 0.49957743 0.4995782  0.49958155 0.49958375\n",
      " 0.49958593 0.49959022 0.49959022 0.49959517 0.49959698 0.49959716\n",
      " 0.49959973 0.49960038 0.49960175 0.49960262 0.49960545 0.49960738\n",
      " 0.4996104  0.49961543 0.499617   0.49961725 0.49961755 0.49962124\n",
      " 0.4996239  0.4996282  0.49963564 0.49964285 0.4996481  0.49964932\n",
      " 0.4996498  0.499652   0.49965376 0.49965808 0.49965873 0.4996604\n",
      " 0.49966133 0.49966177 0.49966243 0.49966517 0.4996661  0.49967116\n",
      " 0.49967247 0.49967533 0.49967557 0.49967623 0.49967694 0.49968123\n",
      " 0.49968213 0.499683   0.49968457 0.49968576 0.4996895  0.4996896\n",
      " 0.49969256 0.4997011  0.49970213 0.4997079  0.49971426 0.4997154\n",
      " 0.49972123 0.4997227  0.4997229  0.49972486 0.49972817 0.49972963\n",
      " 0.49973112 0.49973226 0.49973416 0.49973834 0.49974006 0.49974036\n",
      " 0.4997489  0.49975643 0.49976078 0.49976143 0.4997615  0.49976304\n",
      " 0.49976376 0.49976602 0.49976644 0.49976704 0.49976775 0.49976936\n",
      " 0.4997741  0.49978298 0.4997843  0.49979004 0.49979088 0.49979135\n",
      " 0.4997919  0.4997932  0.49979326 0.49979687 0.49980015 0.4998011\n",
      " 0.49980253 0.49980295 0.49980474 0.499806   0.49980912 0.49981222\n",
      " 0.49981272 0.4998128  0.49981484 0.49981654 0.49981707 0.49981728\n",
      " 0.49982408 0.4998277  0.49983218 0.49983975 0.49984005 0.49984035\n",
      " 0.49984193 0.49984375 0.49984565 0.49984843 0.4998554  0.499863\n",
      " 0.49987012 0.49987087 0.4998761  0.4998796  0.4998822  0.49988475\n",
      " 0.4998859  0.49988738 0.49988753 0.4998886  0.49989033 0.49989155\n",
      " 0.49989897 0.49990016 0.4999014  0.49990144 0.49990615 0.49990958\n",
      " 0.49991018 0.4999118  0.49991772 0.49991992 0.49992296 0.4999236\n",
      " 0.49992427 0.49992552 0.49992627 0.49992713 0.4999273  0.49993086\n",
      " 0.4999309  0.49993274 0.4999337  0.4999406  0.49994174 0.4999425\n",
      " 0.49994412 0.49994484 0.49994755 0.49994755 0.49994823 0.49994954\n",
      " 0.49995124 0.49995306 0.49995443 0.4999545  0.49995485 0.49995536\n",
      " 0.49995667 0.49995813 0.4999601  0.49996316 0.4999636  0.49996796\n",
      " 0.49997073 0.49997142 0.49997196 0.49997196 0.4999731  0.49997547\n",
      " 0.4999755  0.49997962 0.4999822  0.4999834  0.49998602 0.49999094\n",
      " 0.49999878 0.49999946 0.50000435 0.50000536 0.500016   0.50002074\n",
      " 0.50002134 0.50002664 0.5000341  0.5000361  0.5000375  0.5000393\n",
      " 0.5000405  0.50004166 0.50004303 0.50004816 0.50004905 0.5000514\n",
      " 0.50005233 0.5000546  0.5000555  0.50005674 0.500057   0.50005734\n",
      " 0.5000611  0.50006324 0.50006425 0.50006527 0.50006855 0.5000691\n",
      " 0.5000718  0.5000751  0.50007784 0.5000792  0.50007975 0.5000798\n",
      " 0.5000838  0.5000898  0.50009394 0.5000944  0.50009745 0.50010157\n",
      " 0.50010455 0.5001063  0.50011265 0.5001189  0.5001198  0.5001211\n",
      " 0.5001215  0.5001229  0.50012565 0.5001264  0.50013185 0.5001331\n",
      " 0.500136   0.50014186 0.5001427  0.500144   0.5001452  0.5001475\n",
      " 0.5001478  0.50015026 0.50015324 0.50015444 0.50015503 0.5001608\n",
      " 0.500162   0.50016266 0.50016934 0.5001783  0.50017893 0.5001801\n",
      " 0.50018317 0.5001844  0.500186   0.50019    0.500196   0.50019747\n",
      " 0.50019795 0.50020427 0.50021625 0.5002214  0.5002228  0.5002251\n",
      " 0.5002283  0.50022924 0.5002315  0.5002333  0.5002337  0.5002353\n",
      " 0.5002406  0.5002412  0.50024486 0.50024647 0.5002528  0.5002557\n",
      " 0.50025743 0.5002632  0.5002656  0.5002711  0.5002747  0.50027484\n",
      " 0.5002782  0.5002798  0.5002855  0.50028586 0.5002867  0.50028694\n",
      " 0.50029135 0.5002921  0.50029904 0.50029916 0.5003014  0.50030243\n",
      " 0.50030357 0.5003066  0.5003071  0.5003081  0.50031084 0.5003172\n",
      " 0.50032187 0.5003222  0.5003225  0.5003269  0.5003311  0.500332\n",
      " 0.5003375  0.50034475 0.5003471  0.5003485  0.50035    0.5003551\n",
      " 0.5003553  0.5003559  0.5003643  0.50036603 0.50037473 0.50037766\n",
      " 0.500378   0.5003831  0.50039023 0.50039107 0.5003931  0.5003952\n",
      " 0.50039685 0.50040007 0.50040174 0.5004032  0.50040686 0.50040764\n",
      " 0.50041956 0.50041974 0.50042164 0.50042427 0.5004243  0.5004275\n",
      " 0.50042754 0.5004302  0.5004311  0.5004347  0.50043803 0.50044125\n",
      " 0.5004464  0.5004536  0.50045425 0.50045484 0.50045484 0.5004578\n",
      " 0.500458   0.50045884 0.50046057 0.50046164 0.5004644  0.50046515\n",
      " 0.5004665  0.500473   0.5004763  0.5004806  0.5004809  0.5004818\n",
      " 0.50048274 0.5004909  0.50049174 0.50049955 0.5005033  0.50050557\n",
      " 0.500506   0.50051844 0.50051904 0.50052035 0.5005272  0.50052774\n",
      " 0.50053394 0.5005377  0.50053835 0.50054246 0.5005451  0.5005485\n",
      " 0.50054854 0.50055623 0.50056505 0.50057244 0.50057256 0.5005771\n",
      " 0.5005785  0.50057876 0.50058144 0.50059074 0.5005991  0.50060064\n",
      " 0.5006124  0.50061655 0.50061977 0.50062233 0.500623   0.5006239\n",
      " 0.500625   0.50063676 0.50063795 0.5006423  0.5006424  0.50064504\n",
      " 0.50064677 0.50064826 0.5006583  0.50065845 0.5006588  0.5006633\n",
      " 0.50066435 0.5006733  0.50068104 0.50068367 0.5006839  0.50068533\n",
      " 0.50068873 0.5006891  0.50069314 0.5006954  0.5006962  0.5007003\n",
      " 0.5007044  0.50071156 0.5007128  0.5007134  0.50071675 0.5007193\n",
      " 0.5007231  0.50072676 0.50072765 0.50073665 0.50074035 0.50074947\n",
      " 0.50075525 0.5007655  0.50078297 0.50078666 0.5007906  0.50079155\n",
      " 0.5007972  0.5008006  0.5008166  0.5008178  0.50081825 0.50081897\n",
      " 0.5008191  0.5008239  0.50082517 0.5008296  0.5008314  0.500833\n",
      " 0.50084484 0.50084543 0.5008501  0.50085646 0.5008576  0.50086766\n",
      " 0.5008683  0.5008705  0.5008765  0.50087935 0.5008794  0.50088274\n",
      " 0.50088423 0.50088555 0.5008994  0.5009017  0.50090575 0.5009146\n",
      " 0.5009159  0.5009212  0.5009241  0.5009293  0.5009295  0.5009487\n",
      " 0.5009574  0.5009574  0.50095886 0.5009593  0.5009616  0.50096184\n",
      " 0.500962   0.5009769  0.50097746 0.50098485 0.5009909  0.50099117\n",
      " 0.50099325 0.50099456 0.5009962  0.5010027  0.5010047  0.50100946\n",
      " 0.50101554 0.5010164  0.50102204 0.50102717 0.50103015 0.5010364\n",
      " 0.5010399  0.50104    0.5010477  0.5010498  0.50106925 0.5010694\n",
      " 0.5010724  0.5010803  0.50108624 0.5010871  0.5010895  0.50109476\n",
      " 0.5010979  0.50110424 0.5011114  0.50111806 0.50112027 0.5011219\n",
      " 0.50112635 0.50112695 0.5011296  0.5011336  0.50113404 0.5011395\n",
      " 0.5011484  0.5011556  0.50116086 0.50116986 0.5011814  0.50118846\n",
      " 0.50118893 0.50119    0.5012053  0.5012068  0.5012164  0.5012182\n",
      " 0.50122154 0.5012268  0.5012388  0.50124085 0.5012427  0.5012621\n",
      " 0.5012721  0.50127476 0.5013005  0.50131494 0.5013154  0.50131965\n",
      " 0.5013265  0.5013511  0.50135535 0.5013584  0.501373   0.50137526\n",
      " 0.5013783  0.50138605 0.5013946  0.50139564 0.50140435 0.5014344\n",
      " 0.5014348  0.5014467  0.5014628  0.50146306 0.50147307 0.5014772\n",
      " 0.5014961  0.50150985 0.5015373  0.50155854 0.50156605 0.5015669\n",
      " 0.5015738  0.5015849  0.50160116 0.50160295 0.50160307 0.5016313\n",
      " 0.5016413  0.5016647  0.501707   0.50171053 0.5017226  0.5017259\n",
      " 0.50173247 0.5017351  0.5017627  0.50176454 0.50178844 0.5018118\n",
      " 0.5019016  0.5019655  0.50199443 0.502011   0.5020234  0.5020321\n",
      " 0.50205934 0.5020814  0.5021632  0.5023099  0.5023572  0.5025164\n",
      " 0.5025231 ]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 961 is out of bounds for axis 0 with size 961",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-696d976debbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_v_dists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mAP_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_acc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtot_label_occur\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mAP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-7bfe8a3ad429>\u001b[0m in \u001b[0;36mget_acc_score\u001b[1;34m(y_valid, y_q, tot_label_occur)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrecall\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_rank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mtrue_positives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrue_positives\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 961 is out of bounds for axis 0 with size 961"
     ]
    }
   ],
   "source": [
    "for val, camId_v, y_v  in zip(X_val, camId_val, y_val):\n",
    "    v_v_dists = []\n",
    "    y_valid = []\n",
    "    for val2, camId_v2, y_v2  in zip(X_val, camId_val, y_val):\n",
    "        if ((camId_v == camId_v2) and (y_v == y_v2)):\n",
    "            continue\n",
    "        else:\n",
    "            stay = True\n",
    "            while (stay):\n",
    "                randindex = random.randint(0, 965)\n",
    "                if((y_val[randindex] != y_v) and (y_val[randindex] != y_v2)):\n",
    "                    val3 = X_val[randindex]\n",
    "                    y_v3 = y_val[randindex]\n",
    "                    stay = False\n",
    "            dist = model.predict(np.concatenate((val, val2, val3)).reshape((1,6144)))[0][0]\n",
    "            print(dist)\n",
    "            v_v_dists.append(dist)\n",
    "            y_valid.append(y_v3)\n",
    "\n",
    "    tot_label_occur = y_valid.count(y_v)\n",
    "    v_v_dists = np.array(v_v_dists)\n",
    "    y_valid = np.array(y_valid)\n",
    "\n",
    "    \n",
    "    _indexes = np.argsort(v_v_dists)\n",
    "\n",
    "    # Sorted distances and labels\n",
    "    v_v_dists, y_valid = v_v_dists[_indexes], y_valid[_indexes]\n",
    "    print(v_v_dists)\n",
    "\n",
    "    AP_, rank_A = get_acc_score(y_valid, y_v, tot_label_occur)\n",
    "\n",
    "    AP.append(AP_)\n",
    "\n",
    "    rank_accuracies.append(rank_A) \n",
    "\n",
    "    #if q  > 5:\n",
    "    #    break\n",
    "    #q = q+1\n",
    "\n",
    "rank_accuracies = np.array(rank_accuracies)\n",
    "\n",
    "total = rank_accuracies.shape[0]\n",
    "rank_accuracies = rank_accuracies.sum(axis = 0)\n",
    "rank_accuracies = np.divide(rank_accuracies, total)\n",
    "\n",
    "i = 0\n",
    "print ('Accuracies by Rank:')\n",
    "while i < rank_accuracies.shape[0]:\n",
    "    print('Rank ', i+1, ' = %.2f%%' % (rank_accuracies[i] * 100), '\\t',\n",
    "          'Rank ', i+2, ' = %.2f%%' % (rank_accuracies[i+1] * 100), '\\t',\n",
    "          'Rank ', i+3, ' = %.2f%%' % (rank_accuracies[i+2] * 100), '\\t',\n",
    "          'Rank ', i+4, ' = %.2f%%' % (rank_accuracies[i+3] * 100), '\\t',\n",
    "          'Rank ', i+5, ' = %.2f%%' % (rank_accuracies[i+4] * 100))\n",
    "    i = i+5\n",
    "\n",
    "AP = np.array(AP)\n",
    "\n",
    "mAP = AP.sum()/AP.shape[0]\n",
    "print('mAP = %.2f%%' % (mAP * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
